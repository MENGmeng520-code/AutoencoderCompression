{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AgfbhQgEBwDK",
        "YIVgQcFFWK92",
        "NvbLYPBCNk8i",
        "2HhgpWT-hcGo"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27YoqKEigTO9"
      },
      "source": [
        "##Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5GUMp3sZ45I"
      },
      "source": [
        "!pip install tensorflow-compression\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPygakGJid_d"
      },
      "source": [
        "! git clone https://github.com/bckenstler/CLR\n",
        "! ls 'CLR'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ReY2NTxZ74J"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_compression import GDN\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Add, Subtract, Lambda, Multiply, Softmax, Input, Reshape, Reshape, Softmax\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from CLR.clr_callback import CyclicLR\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwqcKDSHUcSS"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Hz1-P9gecG"
      },
      "source": [
        "##Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2hV_KDngh6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897ade9b-7565-47f1-bb63-2f03d7130a89"
      },
      "source": [
        "class DataGenerator(Sequence):\n",
        "    \"\"\"Generates data for Keras\"\"\"\n",
        "\n",
        "    def __init__(self, list_IDs, batch_size=32, dim=(256, 256), n_channels=3,\n",
        "                 n_classes=0, shuffle=True):\n",
        "        \"\"\"Initialization\"\"\"\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        \"\"\"Generates data containing batch_size samples\"\"\"  # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "\n",
        "            X[i,] = np.asarray(resize(imread(ID), (self.dim[0], self.dim[1], self.n_channels)))\n",
        "            y = X\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "def convolutional_block(X, out_ch, downsample, actv2):\n",
        "    \"\"\"\n",
        "    Implementation of the Residual convolutional block\n",
        "    Skip Connections:\n",
        "    They mitigate the problem of vanishing gradient by allowing the alternate shortcut path for gradient to flow through\n",
        "    They allow the model to learn an identity function which ensures that the higher layer will perform at least as good as the lower layer, and not worse\n",
        "\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    out_ch -- Number of filters\n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "\n",
        "    https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33#43fa\n",
        "    \"\"\"\n",
        "\n",
        "    stride = 2 if downsample else 1\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path\n",
        "    X = Conv2D(out_ch, kernel_size=3, strides=stride, padding='same',\n",
        "               activation=LeakyReLU(alpha=0.2))(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    if actv2 == 2:\n",
        "        X = Conv2D(out_ch, kernel_size=3, padding='same',\n",
        "                   activation=GDN(inverse=True))(X)\n",
        "    if actv2 == 1:\n",
        "        X = Conv2D(out_ch, kernel_size=3, padding='same',\n",
        "                   activation=GDN(inverse=False))(X)\n",
        "    else:\n",
        "        X = Conv2D(out_ch, kernel_size=3, padding='same')(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### (â‰ˆ2 lines) if downsample handles different channels, replace by identity if not needed\n",
        "    # if downsample:\n",
        "    X_shortcut = Conv2D(out_ch, kernel_size=(1, 1), strides=stride, padding='same')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def NLAM(X, out_ch, downsample):\n",
        "    \"\"\"\n",
        "    Non-Local Attention Module\n",
        "    Attention modules are used to make CNN learn and focus more on the important information,\n",
        "    rather than learning non-useful background information. In the case of object detection,\n",
        "    useful information is the objects or target class crop that we want to classify and localize in an image.\n",
        "    The attention module consists of a simple 2D-convolutional layer, MLP(in the case of channel attention),\n",
        "    and sigmoid function at the end to generate a mask of the input feature map.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Internal Parameters\n",
        "    actv2 = 1\n",
        "\n",
        "    stride = 2 if downsample else 1\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "    X_main = X\n",
        "\n",
        "    # Mask Block\n",
        "    X = convolutional_block(X, out_ch, downsample, actv2)\n",
        "    X = convolutional_block(X, out_ch, downsample, actv2)\n",
        "    X = convolutional_block(X, out_ch, downsample, actv2)\n",
        "    # Conv Layer\n",
        "    X = Conv2D(out_ch, kernel_size=1, strides=1, padding='same', activation='sigmoid')(X)\n",
        "\n",
        "    # Main Block\n",
        "    X_main = convolutional_block(X_main, out_ch, downsample, actv2)\n",
        "    X_main = convolutional_block(X_main, out_ch, downsample, actv2)\n",
        "    X_main = convolutional_block(X_main, out_ch, downsample, actv2)\n",
        "\n",
        "    # Final\n",
        "    # if downsample:\n",
        "    X_shortcut = Conv2D(out_ch, kernel_size=(1, 1), strides=stride ** 3, padding='same')(X_shortcut)\n",
        "    Y = Multiply()([X, X_main])\n",
        "    Y = Add()([Y, X_shortcut])\n",
        "\n",
        "    return Y\n",
        "\n",
        "\n",
        "def UpSample(X, out_ch):\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    # Path 1\n",
        "    X = Conv2D(4 * out_ch, kernel_size=3, padding='same', activation=LeakyReLU(alpha=0.2))(X)\n",
        "    X = Lambda(lambda x: tf.nn.depth_to_space(x, 2))(X)\n",
        "    X = Conv2D(out_ch, kernel_size=3, padding='same', activation=GDN(inverse=True))(X)\n",
        "\n",
        "    # Path 2\n",
        "    X_shortcut = Conv2D(4 * out_ch, kernel_size=1, padding='same')(X_shortcut)\n",
        "\n",
        "    X_shortcut = Lambda(lambda x: tf.nn.depth_to_space(x, 2))(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path\n",
        "    Y = Add()([X, X_shortcut])\n",
        "\n",
        "    return Y\n",
        "\n",
        "\n",
        "def Quantize(X):\n",
        "    # Mode 0 = Training, 1 = Inference\n",
        "\n",
        "    X_shape = tf.shape(X, out_type=tf.dtypes.int32)\n",
        "\n",
        "    return Lambda(lambda x: tf.math.round(x), name='Inference_Quantizer')(X), Add(name='Training_Quantizer')(\n",
        "        [X, tf.random.uniform(X_shape, minval=-0.5, maxval=0.5, dtype=tf.dtypes.float32)])\n",
        "\n",
        "\n",
        "\n",
        "def SSIM_MAE_VGG_Loss(y_true, y_pred):\n",
        "    alpha = 0.4\n",
        "\n",
        "    mae = tf.reduce_mean(tf.math.abs(y_true - y_pred))\n",
        "    ssim = 1 - tf.reduce_mean(tf.image.ssim_multiscale(y_pred, y_true, 1.0))\n",
        "    vgg = vgg_loss(y_true, y_pred)\n",
        "\n",
        "    return alpha * ssim + (1 - alpha) * mae + vgg \n",
        "\n",
        "\n",
        "def PSNR(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.image.psnr(y_true, y_pred, 1.0), name='PSNR')\n",
        "\n",
        "\n",
        "vgg = VGG19(weights=\"imagenet\", include_top=False, input_shape=(None, None, 3))\n",
        "vgg.outputs = [vgg.get_layer('block5_conv4').output]\n",
        "vgg_model = tf.keras.models.Model(inputs=vgg.inputs, outputs=vgg.outputs)\n",
        "vgg_model.trainable = False\n",
        "\n",
        "\n",
        "def vgg_loss(y_true, y_pred):\n",
        "    return tf.math.reduce_mean(tf.math.abs((vgg_model(y_true)[0] - vgg_model(y_pred)[0])), axis=-1)\n",
        "\n",
        "\n",
        "class MaskConv2D(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_shape, channels=384, k_size=7):\n",
        "        super().__init__()\n",
        "        self.conv = Conv2D(filters=channels, kernel_size=k_size, padding='same', activation=LeakyReLU(alpha=0.2))\n",
        "        k = k_size\n",
        "        self.conv.build(input_shape)\n",
        "        self.channels = channels\n",
        "        self.k_size = k_size\n",
        "        mask = np.ones((k, k), dtype=np.float32)\n",
        "        mask[k // 2, k // 2:] = 0\n",
        "        mask[k // 2 + 1:, :] = 0\n",
        "        mask = np.expand_dims(mask, -1)\n",
        "        mask = np.expand_dims(mask, -1)\n",
        "\n",
        "        self.conv.weights[0].assign(self.conv.weights[0] * mask)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'input_shape': self.input_shape,\n",
        "                  'channels': self.channels,\n",
        "                  'k_size': self.k_size}\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.conv(inputs)\n",
        "\n",
        "\n",
        "def Parameter_Estimate(X, gmm_K):\n",
        "\n",
        "    Y = MaskConv2D(input_shape=X.shape, channels=384, k_size=5)(X)\n",
        "    Y = Conv2D(640, kernel_size=1, padding='same', activation=LeakyReLU(alpha=0.2))(Y)\n",
        "    Y = Conv2D(640, kernel_size=1, padding='same', activation=LeakyReLU(alpha=0.2))(Y)\n",
        "    Y = Conv2D(3 * gmm_K * X.shape[-1], kernel_size=1, padding='same', name='GMM')(Y)\n",
        "\n",
        "    return Y\n",
        "\n",
        "\n",
        "def bitcost(P, Y_hat):\n",
        "\n",
        "    gmm_K = 3\n",
        "    batch, dim1, dim2, dim3 =tf.shape(Y_hat)\n",
        "\n",
        "    P = tf.reshape(P,(-1,dim1, dim2, gmm_K, 3 * dim3))\n",
        "\n",
        "\n",
        "    mu = tf.slice(P,[0,0,0,0,0],[batch,dim1,dim2,gmm_K,dim3])\n",
        "    std_dev = tf.slice(P,[0,0,0,0,dim3],[batch,dim1,dim2,gmm_K,dim3])\n",
        "    weights = tf.slice(P,[0,0,0,0,2 * dim3],[batch,dim1,dim2,gmm_K,dim3])\n",
        "    weights = Softmax(axis=3)(weights)\n",
        "    total_diff = 1e-6\n",
        "\n",
        "    for k in range(gmm_K):\n",
        "        #slicing\n",
        "        weight_k = weights[:, :, :, k, :]\n",
        "        mu_k = mu[:, :, :, k, :]\n",
        "\n",
        "        std_dev_k = Lambda(lambda x:tf.math.abs(x))(std_dev[:, :, :, k, :])\n",
        "\n",
        "        std_dev_k = std_dev_k+1e-6\n",
        "        std_dev_rec = Lambda(lambda x:tf.math.reciprocal(x))(std_dev_k)\n",
        "        #constants\n",
        "\n",
        "        denom = Lambda(lambda x:tf.math.divide(x,tf.math.sqrt(2.0)))(std_dev_rec)\n",
        "\n",
        "        Y_hat_ = Subtract()([Y_hat,mu_k])\n",
        "        #upper limit\n",
        "        upper_y = Y_hat_+0.5\n",
        "        upper_y = Multiply()([upper_y,denom])\n",
        "        erf = Lambda(lambda x:tf.math.erf(x))(upper_y)\n",
        "        upper_limit = 1+erf\n",
        "        upper_limit = upper_limit*0.5\n",
        "\n",
        "\n",
        "        #upper limit\n",
        "        lower_y = Y_hat_-0.5\n",
        "        lower_y = Multiply()([lower_y,denom])\n",
        "        erf = Lambda(lambda x:tf.math.erf(x))(lower_y)\n",
        "        lower_limit = 1+erf\n",
        "        lower_limit = lower_limit*0.5\n",
        "\n",
        "\n",
        "        diff = Subtract()([upper_limit,lower_limit])\n",
        "        diff = Multiply()([diff,weight_k])\n",
        "        total_diff = total_diff+diff\n",
        "\n",
        "        \n",
        "\n",
        "    return -(tf.math.log(total_diff)) / (tf.math.log(2.0) * 256 * tf.cast(dim1 * dim2,dtype=tf.float32))\n",
        "\n",
        "\n",
        "def Bit_Loss(y_true, y_pred):\n",
        "    sum = tf.reduce_sum(y_pred)\n",
        "    target = 0\n",
        "    return tf.math.abs(sum-target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuqQ7OX4gZfH"
      },
      "source": [
        "##Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOXwpQbKtL_t",
        "outputId": "776f562d-55b1-466f-98f0-a56557485413"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV2R5HsW-RbW"
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvh3BF3q_3Ov"
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/val2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRmrKjdpA9qU"
      },
      "source": [
        "!mkdir /content/coco_dataset\n",
        "!unzip -q /content/train2017.zip -d /content/coco_dataset \n",
        "!unzip -q /content/val2017.zip -d /content/coco_dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8_VavRBKCen",
        "outputId": "5e11ebb7-6f88-4d98-d770-dec51ef45dae"
      },
      "source": [
        "import os\n",
        "import random \n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET_DIR = os.path.join(\"/content/coco_dataset\", \"\")\n",
        "TRAIN_DIR = os.path.join(DATASET_DIR, \"train2017\")\n",
        "VAL_DIR = os.path.join(DATASET_DIR, \"val2017\")\n",
        "#TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
        "\n",
        "training_data = [y for x in os.walk(TRAIN_DIR) for y in glob(os.path.join(x[0], '*jpg'))]\n",
        "validation_data = [y for x in os.walk(VAL_DIR) for y in glob(os.path.join(x[0], '*jpg'))]\n",
        "#test_data = [y for x in os.walk(TEST_DIR) for y in glob(os.path.join(x[0], '*JPEG'))]\n",
        "print(len(validation_data))\n",
        "training_data[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/coco_dataset/train2017/000000015693.jpg',\n",
              " '/content/coco_dataset/train2017/000000178218.jpg',\n",
              " '/content/coco_dataset/train2017/000000397893.jpg',\n",
              " '/content/coco_dataset/train2017/000000291941.jpg',\n",
              " '/content/coco_dataset/train2017/000000388311.jpg',\n",
              " '/content/coco_dataset/train2017/000000501026.jpg',\n",
              " '/content/coco_dataset/train2017/000000207010.jpg',\n",
              " '/content/coco_dataset/train2017/000000146988.jpg',\n",
              " '/content/coco_dataset/train2017/000000379517.jpg',\n",
              " '/content/coco_dataset/train2017/000000243557.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fFE_SR3yk7g"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "HEIGHT = 512\n",
        "WIDTH = 512\n",
        "params = {'dim': (HEIGHT, WIDTH),\n",
        "          'batch_size': BATCH_SIZE,\n",
        "          'shuffle': False}\n",
        "training_generator = DataGenerator(training_data, **params)\n",
        "validation_generator = DataGenerator(validation_data, **params)\n",
        "#test_generator = DataGenerator(test_data, **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgfbhQgEBwDK"
      },
      "source": [
        "##Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ-I5GI3B0Aj"
      },
      "source": [
        "ch = 192\n",
        "out_ch = 32\n",
        "img_input = Input(shape=(None, None, 3),name='Image')\n",
        "\n",
        "ResBlock_1 = convolutional_block(img_input, ch, 1, 1)\n",
        "ResBlock_2 = convolutional_block(ResBlock_1, ch, 0, 1)\n",
        "ResBlock_3 = convolutional_block(ResBlock_2, ch, 1, 1)\n",
        "NLAM_1 = NLAM(ResBlock_3, ch, 0)\n",
        "ResBlock_4 = convolutional_block(NLAM_1, ch, 0, 1)\n",
        "ResBlock_5 = convolutional_block(ResBlock_4, ch, 1, 1)\n",
        "ResBlock_6 = convolutional_block(ResBlock_5, ch, 0, 1)\n",
        "NLAM_2 = NLAM(ResBlock_6, ch, 0)\n",
        "final_conv2d = Conv2D(out_ch, kernel_size=3, strides=2, padding='same',\n",
        "                      activation=LeakyReLU(alpha=0.2))(NLAM_2)\n",
        "\n",
        "y_hard , y_soft = Quantize(final_conv2d)\n",
        "    \n",
        "Encoder = Model(inputs=img_input,outputs=[y_hard , y_soft],name='Encoder')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIVgQcFFWK92"
      },
      "source": [
        "##ÙŽEntropy Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fwZX2VUWOQH"
      },
      "source": [
        "latent_code = Input(shape=(None, None, out_ch),name='Latent_Code')\n",
        "params = Parameter_Estimate(latent_code,3)\n",
        "bits_per_pixel = bitcost(params,latent_code)\n",
        "Entropy_Model=Model(inputs=latent_code,outputs=[bits_per_pixel,params],name='Entropy_Model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvbLYPBCNk8i"
      },
      "source": [
        "##Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhYguGAiNotu"
      },
      "source": [
        "decoder_input = Input(shape=(None, None, out_ch),name='Decoder_Input')\n",
        "conv2d_1 = Conv2D(ch, kernel_size=3, padding='same',\n",
        "                      activation=LeakyReLU(alpha=0.2))(decoder_input)\n",
        "NLAM_1 = NLAM(conv2d_1, ch, 0)\n",
        "ResBlock_1 = convolutional_block(NLAM_1, ch, 0, 2)\n",
        "UpResBlock_1 = UpSample(ResBlock_1, ch)\n",
        "ResBlock_2 = convolutional_block(UpResBlock_1, ch, 0, 2)\n",
        "UpResBlock_2 = UpSample(ResBlock_2, ch)\n",
        "NLAM_2 = NLAM(UpResBlock_2, ch, 0)\n",
        "ResBlock_3 = convolutional_block(NLAM_2, ch, 0, 2)\n",
        "UpResBlock_3 = UpSample(ResBlock_3, ch)\n",
        "ResBlock_4 = convolutional_block(UpResBlock_3, ch, 0, 2)\n",
        "conv2d_2 = Conv2D(12, kernel_size=3, padding='same')(ResBlock_4)\n",
        "\n",
        "UpResBlock_4 = UpSample(conv2d_2, 3)\n",
        "\n",
        "Decoder = Model(inputs=decoder_input, outputs = UpResBlock_4,name='Decoder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HhgpWT-hcGo"
      },
      "source": [
        "##End to End Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMWzjIn5hfqI"
      },
      "source": [
        "autoencoder_input = Input(shape=(None, None, 3),name='Image')\n",
        "encoder_output = Encoder(autoencoder_input)[1]\n",
        "bpp = Entropy_Model(encoder_output)[0]\n",
        "\n",
        "decoder_output = Decoder(encoder_output)\n",
        "    \n",
        "autoencoder = Model(inputs=autoencoder_input,\n",
        "                    outputs=[bpp,decoder_output],\n",
        "                    name='End_to_End')\n",
        "\n",
        "\n",
        "autoencoder.build((None, None, 3))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "autoencoder.compile(optimizer=opt,\n",
        "                    loss=[Bit_Loss,SSIM_MAE_VGG_Loss],\n",
        "                    loss_weights=[0.1,0.9],\n",
        "                    metrics={'Decoder':PSNR})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "OUaE2VbrjDyR",
        "outputId": "76d14f52-4b6b-4289-a636-5cd11be82425"
      },
      "source": [
        " tf.keras.utils.plot_model(autoencoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXxt4mothzhO"
      },
      "source": [
        "#Callbacks and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsnve6dm4xW6"
      },
      "source": [
        "#autoencoder.load_weights('/content/model/MyDrive/cv_models/SSIM_MAE_VGG_COCO_model.h5')\n",
        "clr = CyclicLR(base_lr=1e-5, max_lr=1e-4, step_size=781, gamma=0.99994)\n",
        "chk_point = tf.keras.callbacks.ModelCheckpoint(filepath='/content/model/MyDrive/cv_models/SSIM_MAE_VGG_ADV_COCO_model.{epoch:02d}.h5')\n",
        "ton = tf.keras.callbacks.TerminateOnNaN()\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "history = autoencoder.fit(training_generator,\n",
        "                      callbacks = [clr,ton,chk_point],\n",
        "                      validation_data=validation_generator,\n",
        "                      epochs=NUM_EPOCHS)\n",
        "autoencoder.save('/content/model/MyDrive/cv_models/SSIM_MAE_VGG_COCO_ADV_model.h5',save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}